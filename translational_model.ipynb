{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation model from COCO -> MPII\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datagrid/personal/baljibil/repos/env-vitpose/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ROOT_FOLDER = \"/datagrid/personal/baljibil\"\n",
    "# ground truth\n",
    "with open(ROOT_FOLDER + '/data/MPII_COCO/annotations/mpii_val.json', 'r') as f:\n",
    "    annot_truth = json.load(f)\n",
    "# prediction\n",
    "with open(ROOT_FOLDER + '/repos/scripts/coco_pred_on_mpii/pred.json', 'r') as f:\n",
    "    annot_pred_coco = json.load(f)\n",
    "\n",
    "# with open(ROOT_FOLDER + '/repos/scripts/mpii_pred_on_mpii/pred.json', 'r') as f:\n",
    "#     annot_pred_mpii = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (2958, 17, 2)\n",
      "y shape: (2958, 16, 2)\n",
      "box scale: (2958,)\n",
      "box center: (2958, 2)\n"
     ]
    }
   ],
   "source": [
    "X = list()\n",
    "y = list()\n",
    "box_scale = list()\n",
    "box_center = list()\n",
    "for i in range(len(annot_truth)):\n",
    "  X.append(annot_pred_coco['annotations'][i]['joints'])\n",
    "  y.append(annot_truth[i]['joints'])\n",
    "  box_scale.append(annot_truth[i]['scale'])\n",
    "  box_center.append(annot_truth[i]['center'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "box_scale = np.array(box_scale)\n",
    "box_center = np.array(box_center)\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"box scale:\", box_scale.shape)\n",
    "print(\"box center:\", box_center.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset\n",
    "- First by scale value of the bounding box\n",
    "- Then either 0-1 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_normalized = X - np.repeat(box_center[:, np.newaxis, :], 17, axis=1)\n",
    "X_normalized = X_normalized / box_scale[:, np.newaxis, np.newaxis]\n",
    "y_normalized = y - np.repeat(box_center[:, np.newaxis, :], 16, axis=1)\n",
    "y_normalized = y_normalized / box_scale[:, np.newaxis, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train instance:  1552\n",
      "Val instance:  518\n",
      "Test instance:  888\n",
      "torch.Size([1552, 17, 2])\n",
      "torch.Size([1552, 16, 2])\n",
      "torch.Size([518, 17, 2])\n",
      "torch.Size([518, 16, 2])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_normalized, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Train instance: \", X_train.shape[0])\n",
    "print(\"Val instance: \", X_val.shape[0])\n",
    "print(\"Test instance: \", X_test.shape[0])\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  tensor([[[ 2.4119, -0.3988],\n",
      "         [ 0.1322,  3.8371],\n",
      "         [-5.0854, -1.9829],\n",
      "         [-0.5996, -4.2318],\n",
      "         [ 2.5422,  3.2279],\n",
      "         [-0.5547, -0.2550],\n",
      "         [-1.4770,  1.6041],\n",
      "         [-3.2899,  2.5912],\n",
      "         [-4.0852,  0.2372],\n",
      "         [-2.3274,  0.3135],\n",
      "         [ 3.7015, -1.5642],\n",
      "         [-0.9739,  2.6098],\n",
      "         [-2.3990,  3.3872],\n",
      "         [ 2.6193, -0.8078],\n",
      "         [-2.6423, -0.9959],\n",
      "         [-2.0022,  0.6103]]])\n",
      "GT: tensor([[[-23.0007,  99.1904],\n",
      "         [-21.5631,  46.0013],\n",
      "         [-20.1256,  -0.4792],\n",
      "         [  4.3126,  -0.4792],\n",
      "         [  5.2710,  46.9597],\n",
      "         [  9.5836, 100.6279],\n",
      "         [ -7.6669,  -0.4792],\n",
      "         [ -7.1877, -46.4805],\n",
      "         [ -7.3666, -56.4111],\n",
      "         [ -7.9671, -89.7390],\n",
      "         [-17.7297,   1.4375],\n",
      "         [-30.1884, -23.4798],\n",
      "         [-28.7508, -46.4805],\n",
      "         [ 14.3754, -46.9597],\n",
      "         [ 20.1256, -20.6048],\n",
      "         [  3.8334,  -0.4792]]])\n",
      "Epoch [1/200], Train Loss: 2011.6083, Val Loss: 2014.2563\n",
      "Epoch [2/200], Train Loss: 1520.9937, Val Loss: 1916.3333\n",
      "Epoch [3/200], Train Loss: 1429.6470, Val Loss: 1751.5848\n",
      "Epoch [4/200], Train Loss: 1384.0301, Val Loss: 1677.9535\n",
      "Epoch [5/200], Train Loss: 1334.4116, Val Loss: 1635.1385\n",
      "Epoch [6/200], Train Loss: 1275.9405, Val Loss: 1631.7328\n",
      "Epoch [7/200], Train Loss: 1249.6900, Val Loss: 1603.9324\n",
      "Epoch [8/200], Train Loss: 1229.4222, Val Loss: 1559.1841\n",
      "Epoch [9/200], Train Loss: 1211.1827, Val Loss: 1573.1078\n",
      "Epoch [10/200], Train Loss: 1219.5776, Val Loss: 1557.7211\n",
      "Epoch [11/200], Train Loss: 1234.4658, Val Loss: 1598.0954\n",
      "Epoch [12/200], Train Loss: 1200.8890, Val Loss: 1563.1625\n",
      "Epoch [13/200], Train Loss: 1153.9467, Val Loss: 1549.6382\n",
      "Epoch [14/200], Train Loss: 1149.3082, Val Loss: 1540.2886\n",
      "Epoch [15/200], Train Loss: 1135.0263, Val Loss: 1519.1498\n",
      "Epoch [16/200], Train Loss: 1136.3970, Val Loss: 1546.3060\n",
      "Epoch [17/200], Train Loss: 1126.5362, Val Loss: 1583.0895\n",
      "Epoch [18/200], Train Loss: 1167.8037, Val Loss: 1544.4897\n",
      "Epoch [19/200], Train Loss: 1123.7993, Val Loss: 1545.2019\n",
      "Epoch [20/200], Train Loss: 1082.7796, Val Loss: 1506.9890\n",
      "Epoch [21/200], Train Loss: 1061.5209, Val Loss: 1521.2126\n",
      "Epoch [22/200], Train Loss: 1039.6400, Val Loss: 1615.1250\n",
      "Epoch [23/200], Train Loss: 1078.6570, Val Loss: 1583.3935\n",
      "Epoch [24/200], Train Loss: 1050.7647, Val Loss: 1508.3434\n",
      "Epoch [25/200], Train Loss: 1008.5631, Val Loss: 1569.1127\n",
      "Epoch [26/200], Train Loss: 1019.5830, Val Loss: 1583.4147\n",
      "Epoch [27/200], Train Loss: 1023.0372, Val Loss: 1643.8067\n",
      "Epoch [28/200], Train Loss: 1007.9847, Val Loss: 1611.6714\n",
      "Epoch [29/200], Train Loss: 1003.9448, Val Loss: 1608.6218\n",
      "Epoch [30/200], Train Loss: 980.9933, Val Loss: 1627.3613\n",
      "Epoch [31/200], Train Loss: 1004.0838, Val Loss: 1638.7950\n",
      "Epoch [32/200], Train Loss: 983.6848, Val Loss: 1619.1500\n",
      "Epoch [33/200], Train Loss: 984.1696, Val Loss: 1605.6330\n",
      "Epoch [34/200], Train Loss: 955.2224, Val Loss: 1611.6017\n",
      "Epoch [35/200], Train Loss: 916.1137, Val Loss: 1593.9698\n",
      "Epoch [36/200], Train Loss: 934.3409, Val Loss: 1637.9062\n",
      "Epoch [37/200], Train Loss: 924.1431, Val Loss: 1570.1974\n",
      "Epoch [38/200], Train Loss: 911.2174, Val Loss: 1627.3307\n",
      "Epoch [39/200], Train Loss: 903.7844, Val Loss: 1622.5311\n",
      "Epoch [40/200], Train Loss: 904.7450, Val Loss: 1668.7302\n",
      "Epoch [41/200], Train Loss: 915.3679, Val Loss: 1649.7264\n",
      "Epoch [42/200], Train Loss: 935.3584, Val Loss: 1651.7482\n",
      "Epoch [43/200], Train Loss: 921.5372, Val Loss: 1641.8267\n",
      "Epoch [44/200], Train Loss: 910.0345, Val Loss: 1705.7119\n",
      "Epoch [45/200], Train Loss: 899.8049, Val Loss: 1653.6170\n",
      "Epoch [46/200], Train Loss: 887.7995, Val Loss: 1660.8854\n",
      "Epoch [47/200], Train Loss: 886.8068, Val Loss: 1615.3923\n",
      "Epoch [48/200], Train Loss: 923.3500, Val Loss: 1598.4119\n",
      "Epoch [49/200], Train Loss: 902.2978, Val Loss: 1622.7621\n",
      "Epoch [50/200], Train Loss: 871.0651, Val Loss: 1618.2031\n",
      "Epoch [51/200], Train Loss: 857.0741, Val Loss: 1586.5769\n",
      "Epoch [52/200], Train Loss: 855.0736, Val Loss: 1624.8910\n",
      "Epoch [53/200], Train Loss: 891.0680, Val Loss: 1681.1283\n",
      "Epoch [54/200], Train Loss: 911.2025, Val Loss: 1582.0937\n",
      "Epoch [55/200], Train Loss: 864.5033, Val Loss: 1602.7511\n",
      "Epoch [56/200], Train Loss: 875.0600, Val Loss: 1652.6047\n",
      "Epoch [57/200], Train Loss: 864.7580, Val Loss: 1609.3914\n",
      "Epoch [58/200], Train Loss: 854.4517, Val Loss: 1636.8198\n",
      "Epoch [59/200], Train Loss: 811.9915, Val Loss: 1611.4220\n",
      "Epoch [60/200], Train Loss: 798.5338, Val Loss: 1638.8620\n",
      "Epoch [61/200], Train Loss: 810.2859, Val Loss: 1661.3692\n",
      "Epoch [62/200], Train Loss: 798.2992, Val Loss: 1663.3590\n",
      "Epoch [63/200], Train Loss: 791.5148, Val Loss: 1612.2757\n",
      "Epoch [64/200], Train Loss: 795.5979, Val Loss: 1617.5089\n",
      "Epoch [65/200], Train Loss: 781.4527, Val Loss: 1657.1522\n",
      "Epoch [66/200], Train Loss: 772.1118, Val Loss: 1627.0278\n",
      "Epoch [67/200], Train Loss: 781.3598, Val Loss: 1665.4908\n",
      "Epoch [68/200], Train Loss: 774.6843, Val Loss: 1646.0285\n",
      "Epoch [69/200], Train Loss: 792.2469, Val Loss: 1659.0573\n",
      "Epoch [70/200], Train Loss: 786.8765, Val Loss: 1633.1614\n",
      "Epoch [71/200], Train Loss: 785.3193, Val Loss: 1632.3392\n",
      "Epoch [72/200], Train Loss: 840.1059, Val Loss: 1681.1951\n",
      "Epoch [73/200], Train Loss: 793.9088, Val Loss: 1673.2236\n",
      "Epoch [74/200], Train Loss: 830.0757, Val Loss: 1614.6061\n",
      "Epoch [75/200], Train Loss: 811.3653, Val Loss: 1623.2022\n",
      "Epoch [76/200], Train Loss: 776.6387, Val Loss: 1659.6544\n",
      "Epoch [77/200], Train Loss: 802.4979, Val Loss: 1616.8007\n",
      "Epoch [78/200], Train Loss: 762.2759, Val Loss: 1664.0645\n",
      "Epoch [79/200], Train Loss: 752.3636, Val Loss: 1625.4838\n",
      "Epoch [80/200], Train Loss: 757.1098, Val Loss: 1644.4644\n",
      "Epoch [81/200], Train Loss: 752.3240, Val Loss: 1651.0431\n",
      "Epoch [82/200], Train Loss: 745.8502, Val Loss: 1649.2442\n",
      "Epoch [83/200], Train Loss: 773.3841, Val Loss: 1654.0803\n",
      "Epoch [84/200], Train Loss: 777.7053, Val Loss: 1649.5541\n",
      "Epoch [85/200], Train Loss: 744.5117, Val Loss: 1709.1292\n",
      "Epoch [86/200], Train Loss: 726.1858, Val Loss: 1634.0134\n",
      "Epoch [87/200], Train Loss: 728.3912, Val Loss: 1662.9927\n",
      "Epoch [88/200], Train Loss: 760.8414, Val Loss: 1661.5467\n",
      "Epoch [89/200], Train Loss: 762.4185, Val Loss: 1685.8857\n",
      "Epoch [90/200], Train Loss: 715.5932, Val Loss: 1649.8388\n",
      "Epoch [91/200], Train Loss: 720.8607, Val Loss: 1646.6956\n",
      "Epoch [92/200], Train Loss: 699.6444, Val Loss: 1675.6693\n",
      "Epoch [93/200], Train Loss: 704.1640, Val Loss: 1645.0546\n",
      "Epoch [94/200], Train Loss: 708.4209, Val Loss: 1686.2970\n",
      "Epoch [95/200], Train Loss: 709.3153, Val Loss: 1640.6535\n",
      "Epoch [96/200], Train Loss: 703.1673, Val Loss: 1710.3689\n",
      "Epoch [97/200], Train Loss: 701.4042, Val Loss: 1702.6058\n",
      "Epoch [98/200], Train Loss: 759.5569, Val Loss: 1702.1152\n",
      "Epoch [99/200], Train Loss: 731.5492, Val Loss: 1699.9610\n",
      "Epoch [100/200], Train Loss: 717.8033, Val Loss: 1608.8316\n",
      "Epoch [101/200], Train Loss: 718.5211, Val Loss: 1629.5015\n",
      "Epoch [102/200], Train Loss: 708.5706, Val Loss: 1655.9405\n",
      "Epoch [103/200], Train Loss: 702.6519, Val Loss: 1677.4086\n",
      "Epoch [104/200], Train Loss: 693.1120, Val Loss: 1655.4727\n",
      "Epoch [105/200], Train Loss: 675.3633, Val Loss: 1645.3890\n",
      "Epoch [106/200], Train Loss: 670.3506, Val Loss: 1667.9384\n",
      "Epoch [107/200], Train Loss: 663.4863, Val Loss: 1698.8943\n",
      "Epoch [108/200], Train Loss: 721.2322, Val Loss: 1763.7802\n",
      "Epoch [109/200], Train Loss: 806.8100, Val Loss: 1665.6219\n",
      "Epoch [110/200], Train Loss: 781.4490, Val Loss: 1717.9714\n",
      "Epoch [111/200], Train Loss: 827.7090, Val Loss: 1655.5408\n",
      "Epoch [112/200], Train Loss: 746.9215, Val Loss: 1693.7996\n",
      "Epoch [113/200], Train Loss: 713.3529, Val Loss: 1649.9239\n",
      "Epoch [114/200], Train Loss: 716.3721, Val Loss: 1637.4197\n",
      "Epoch [115/200], Train Loss: 677.2876, Val Loss: 1653.5983\n",
      "Epoch [116/200], Train Loss: 661.8829, Val Loss: 1682.9555\n",
      "Epoch [117/200], Train Loss: 642.1853, Val Loss: 1710.0774\n",
      "Epoch [118/200], Train Loss: 663.5183, Val Loss: 1766.5817\n",
      "Epoch [119/200], Train Loss: 683.1676, Val Loss: 1721.7388\n",
      "Epoch [120/200], Train Loss: 678.8374, Val Loss: 1662.2251\n",
      "Epoch [121/200], Train Loss: 681.5150, Val Loss: 1769.0622\n",
      "Epoch [122/200], Train Loss: 690.3721, Val Loss: 1743.5116\n",
      "Epoch [123/200], Train Loss: 696.9054, Val Loss: 1753.1352\n",
      "Epoch [124/200], Train Loss: 662.6058, Val Loss: 1652.0703\n",
      "Epoch [125/200], Train Loss: 637.8133, Val Loss: 1694.2790\n",
      "Epoch [126/200], Train Loss: 633.6447, Val Loss: 1656.9683\n",
      "Epoch [127/200], Train Loss: 639.2605, Val Loss: 1722.1685\n",
      "Epoch [128/200], Train Loss: 633.5388, Val Loss: 1723.1187\n",
      "Epoch [129/200], Train Loss: 663.6754, Val Loss: 1688.8191\n",
      "Epoch [130/200], Train Loss: 668.5529, Val Loss: 1796.1061\n",
      "Epoch [131/200], Train Loss: 668.8404, Val Loss: 1734.3576\n",
      "Epoch [132/200], Train Loss: 746.8320, Val Loss: 1766.7624\n",
      "Epoch [133/200], Train Loss: 748.3183, Val Loss: 1700.4003\n",
      "Epoch [134/200], Train Loss: 750.3342, Val Loss: 1691.5423\n",
      "Epoch [135/200], Train Loss: 742.8625, Val Loss: 1710.9217\n",
      "Epoch [136/200], Train Loss: 701.5619, Val Loss: 1642.9548\n",
      "Epoch [137/200], Train Loss: 660.5083, Val Loss: 1748.0360\n",
      "Epoch [138/200], Train Loss: 658.5012, Val Loss: 1650.2132\n",
      "Epoch [139/200], Train Loss: 666.3952, Val Loss: 1872.2844\n",
      "Epoch [140/200], Train Loss: 666.2965, Val Loss: 1762.8269\n",
      "Epoch [141/200], Train Loss: 641.9665, Val Loss: 1777.5684\n",
      "Epoch [142/200], Train Loss: 636.0586, Val Loss: 1659.4574\n",
      "Epoch [143/200], Train Loss: 613.7942, Val Loss: 1678.1975\n",
      "Epoch [144/200], Train Loss: 626.7133, Val Loss: 1690.6782\n",
      "Epoch [145/200], Train Loss: 615.6586, Val Loss: 1721.8334\n",
      "Epoch [146/200], Train Loss: 596.1717, Val Loss: 1707.5676\n",
      "Epoch [147/200], Train Loss: 601.0811, Val Loss: 1717.7434\n",
      "Epoch [148/200], Train Loss: 619.7014, Val Loss: 1689.0192\n",
      "Epoch [149/200], Train Loss: 621.3486, Val Loss: 1756.0838\n",
      "Epoch [150/200], Train Loss: 593.6911, Val Loss: 1733.6155\n",
      "Epoch [151/200], Train Loss: 591.1203, Val Loss: 1727.0753\n",
      "Epoch [152/200], Train Loss: 622.8744, Val Loss: 1770.1340\n",
      "Epoch [153/200], Train Loss: 680.0439, Val Loss: 1707.1304\n",
      "Epoch [154/200], Train Loss: 647.4730, Val Loss: 1729.3697\n",
      "Epoch [155/200], Train Loss: 619.3809, Val Loss: 1654.7120\n",
      "Epoch [156/200], Train Loss: 585.8712, Val Loss: 1683.5360\n",
      "Epoch [157/200], Train Loss: 572.7139, Val Loss: 1746.5187\n",
      "Epoch [158/200], Train Loss: 570.1428, Val Loss: 1706.2847\n",
      "Epoch [159/200], Train Loss: 565.3058, Val Loss: 1686.3801\n",
      "Epoch [160/200], Train Loss: 559.4519, Val Loss: 1748.5880\n",
      "Epoch [161/200], Train Loss: 561.3006, Val Loss: 1792.5527\n",
      "Epoch [162/200], Train Loss: 558.4526, Val Loss: 1743.3675\n",
      "Epoch [163/200], Train Loss: 554.7576, Val Loss: 1806.1324\n",
      "Epoch [164/200], Train Loss: 563.8443, Val Loss: 1747.5886\n",
      "Epoch [165/200], Train Loss: 572.5825, Val Loss: 1734.3756\n",
      "Epoch [166/200], Train Loss: 577.8600, Val Loss: 1805.5892\n",
      "Epoch [167/200], Train Loss: 563.6221, Val Loss: 1740.8892\n",
      "Epoch [168/200], Train Loss: 579.8192, Val Loss: 1747.6677\n",
      "Epoch [169/200], Train Loss: 574.9010, Val Loss: 1722.1412\n",
      "Epoch [170/200], Train Loss: 593.7359, Val Loss: 1715.2784\n",
      "Epoch [171/200], Train Loss: 595.4546, Val Loss: 1778.2985\n",
      "Epoch [172/200], Train Loss: 569.7524, Val Loss: 1750.3483\n",
      "Epoch [173/200], Train Loss: 590.5989, Val Loss: 1694.2159\n",
      "Epoch [174/200], Train Loss: 561.2832, Val Loss: 1735.5798\n",
      "Epoch [175/200], Train Loss: 564.4253, Val Loss: 1703.9792\n",
      "Epoch [176/200], Train Loss: 567.6995, Val Loss: 1787.4505\n",
      "Epoch [177/200], Train Loss: 577.3106, Val Loss: 1801.6241\n",
      "Epoch [178/200], Train Loss: 801.9784, Val Loss: 1849.5592\n",
      "Epoch [179/200], Train Loss: 712.3361, Val Loss: 1756.3755\n",
      "Epoch [180/200], Train Loss: 671.8396, Val Loss: 1728.6176\n",
      "Epoch [181/200], Train Loss: 647.4921, Val Loss: 1725.8721\n",
      "Epoch [182/200], Train Loss: 601.1450, Val Loss: 1828.0524\n",
      "Epoch [183/200], Train Loss: 598.5401, Val Loss: 1739.1828\n",
      "Epoch [184/200], Train Loss: 599.4523, Val Loss: 1739.1965\n",
      "Epoch [185/200], Train Loss: 563.5063, Val Loss: 1776.6900\n",
      "Epoch [186/200], Train Loss: 553.9501, Val Loss: 1799.6823\n",
      "Epoch [187/200], Train Loss: 536.7226, Val Loss: 1741.0178\n",
      "Epoch [188/200], Train Loss: 552.1762, Val Loss: 1693.5945\n",
      "Epoch [189/200], Train Loss: 585.8576, Val Loss: 1717.8311\n",
      "Epoch [190/200], Train Loss: 620.9715, Val Loss: 1740.8668\n",
      "Epoch [191/200], Train Loss: 565.6537, Val Loss: 1743.3890\n",
      "Epoch [192/200], Train Loss: 585.4715, Val Loss: 1826.7071\n",
      "Epoch [193/200], Train Loss: 569.5996, Val Loss: 1777.2445\n",
      "Epoch [194/200], Train Loss: 579.3802, Val Loss: 1737.6205\n",
      "Epoch [195/200], Train Loss: 540.2240, Val Loss: 1728.8007\n",
      "Epoch [196/200], Train Loss: 525.5331, Val Loss: 1755.8418\n",
      "Epoch [197/200], Train Loss: 516.2951, Val Loss: 1737.0183\n",
      "Epoch [198/200], Train Loss: 505.4871, Val Loss: 1762.6007\n",
      "Epoch [199/200], Train Loss: 503.1244, Val Loss: 1749.2212\n",
      "Epoch [200/200], Train Loss: 554.8908, Val Loss: 1758.4941\n",
      "TranslatorNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (hidden1): Linear(in_features=34, out_features=64, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=32, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class TranslatorNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden1 = nn.Linear(17 * 2, 64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(64, 32)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(32, 16*2)\n",
    "        # self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x.view(-1, 16, 2)\n",
    "\n",
    "    \n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = TranslatorNN()\n",
    "# print(X_train.shape)\n",
    "with torch.no_grad():\n",
    "    print(\"Pred: \", model(X_train)[:1])\n",
    "    print(\"GT:\", y_train[:1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 200\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_epoch_loss = val_loss / len(val_dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}\")\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
